{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede717c3",
   "metadata": {},
   "source": [
    "## Census Annual Business Survey ETL:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "921d4edb",
   "metadata": {},
   "source": [
    "#### Company Summary "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ad524a0",
   "metadata": {},
   "source": [
    "The first step in our ETL report was to import the modules that were necessary to pull in our dataset. We used the **requests** module to help us send HTTP requests to the Census API's endpoint and retrieve the data. The **pandas** module is used to create a new data frame structure in which we can place and manipulate our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b25a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f422116",
   "metadata": {},
   "source": [
    "The next step is to create a list of *useful_columns* that we want to use as our desired data frame columns. The *get* variable is composed of the dataset columns that we will be pulling from. *get_list* splits *get* on every instance of a comma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b503a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_columns = ['NAME', 'NAICS2017_LABEL','RCPSZFI_LABEL', 'SEX_LABEL', 'ETH_GROUP_LABEL', 'RACE_GROUP_LABEL', 'VET_GROUP_LABEL', 'EMPSZFI_LABEL', 'FIRMPDEMP', 'RCPPDEMP', 'PAYANN', 'YEAR', \"YIBSZFI\", \"EMP\"]\n",
    "get = 'GEO_ID,NAME,NAICS2017,NAICS2017_LABEL,SEX,SEX_LABEL,ETH_GROUP,ETH_GROUP_LABEL,RACE_GROUP,RACE_GROUP_LABEL,VET_GROUP,VET_GROUP_LABEL,EMPSZFI,EMPSZFI_LABEL,YEAR,FIRMPDEMP,FIRMPDEMP_F,RCPPDEMP,RCPPDEMP_F,YIBSZFI,PAYANN,EMP,FIRMPDEMP_S,FIRMPDEMP_S_F,RCPPDEMP_S,RCPPDEMP_S_F,EMP_S,RCPSZFI_LABEL,PAYANN_S,PAYANN_S_F,RCPSZFI'\n",
    "get_list = get.split(',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88a21571",
   "metadata": {},
   "source": [
    "We now need to drop the columns that aren't needed for our data frame. We created an empty list called *columns_to_drop*. We then created a for loop that iterates through *get_list* from above and checks to see if the column is a part of *useful_columns*. We then add the columns not a part of *useful_columns* to our empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b27434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending the less useful columns\n",
    "columns_to_drop = []\n",
    "for items in get_list:\n",
    "    if items in useful_columns:\n",
    "        pass\n",
    "    else:\n",
    "        columns_to_drop.append(items)\n",
    "# When one adds filters to the api, it adds duplicates of the columns to the data.\n",
    "# Instead of renaming those columns (which are auto-filled in Pandas as integers)\n",
    "# we will append the colmun number to drop it later\n",
    "columns_to_drop.append(31) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3b66a40",
   "metadata": {},
   "source": [
    "Now, we will pull the data from the Census. We use a loop to iterate through all the years of data available. We use our API key and census data url to run a get request on the dataset. We rename our headers, and drop the unecessary columns in the *columns_to_drop* list. Our final data frame, *company_summary* is created at the end of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d77afb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MasonLonoff\\AppData\\Local\\Temp\\ipykernel_23580\\292551875.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  company_summary = pd.DataFrame(company_summary.append(company_summary_tmp, ignore_index=True))\n",
      "C:\\Users\\MasonLonoff\\AppData\\Local\\Temp\\ipykernel_23580\\292551875.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  company_summary = pd.DataFrame(company_summary.append(company_summary_tmp, ignore_index=True))\n",
      "C:\\Users\\MasonLonoff\\AppData\\Local\\Temp\\ipykernel_23580\\292551875.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  company_summary = pd.DataFrame(company_summary.append(company_summary_tmp, ignore_index=True))\n"
     ]
    }
   ],
   "source": [
    "for i in range(2017, 2021):\n",
    "    # We establish a base year\n",
    "    if i == 2017:\n",
    "        year = str(i)\n",
    "        key = \"be6437ff702ba92a0aebb292fab267af87d8549c\"\n",
    "        geography = 'us'\n",
    "        url = (f\"https://api.census.gov/data/{year}/abscs?get={get}\" +\n",
    "                f\"&for={geography}:\" +\n",
    "                f\"*&key={key}\")\n",
    "        r = requests.get(url)\n",
    "        text = r.text\n",
    "\n",
    "        # We create a temporary database using the text\n",
    "        df = pd.read_json(text)\n",
    "\n",
    "        # We're now appending the first row of df to an empty list, headers\n",
    "        headers = []\n",
    "        for i in range(0,len(get_list)):\n",
    "            headers.append(df.iloc[0][i])\n",
    "\n",
    "        # Now we will extract the remaining data from df\n",
    "        company_summary = pd.DataFrame(data = df.iloc[1:])\n",
    "\n",
    "        # We will next reset the headers of company_summary\n",
    "        for i in range(0,len(get_list)):\n",
    "            company_summary.rename(columns = {i: headers[i]}, inplace = True)\n",
    "\n",
    "        # Now we will drop the less useful columns\n",
    "        company_summary.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "    else:\n",
    "        # We repeat the previous setps fro next year's data\n",
    "        year = str(i)\n",
    "        key = \"be6437ff702ba92a0aebb292fab267af87d8549c\"\n",
    "        geography = 'us'\n",
    "        url = (f\"https://api.census.gov/data/{year}/abscs?get={get}\" +\n",
    "                f\"&for={geography}:\" +\n",
    "                f\"*&key={key}\")\n",
    "        r = requests.get(url)\n",
    "        text = r.text\n",
    "\n",
    "        df = pd.read_json(text)\n",
    "\n",
    "        company_summary_tmp = pd.DataFrame(data = df.iloc[1:])\n",
    "\n",
    "        for i in range(0,len(get_list)):\n",
    "            company_summary_tmp.rename(columns = {i: headers[i]}, inplace = True)\n",
    "        company_summary_tmp.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "        # Now we will append our dataframe with next year's data\n",
    "        company_summary = pd.DataFrame(company_summary.append(company_summary_tmp, ignore_index=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6db148b1",
   "metadata": {},
   "source": [
    "We now have our data frame with our selected columns. The last step of cleaning our data is to make sure the data have the correct data types. The code below changes certain columns into the correct data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12bea45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_summary['YEAR'] = company_summary['YEAR'].astype(int)\n",
    "company_summary['FIRMPDEMP'] = company_summary['FIRMPDEMP'].astype(int)\n",
    "company_summary['RCPPDEMP'] = company_summary['RCPPDEMP'].astype(float)\n",
    "company_summary['EMP'] = company_summary['EMP'].astype(int)\n",
    "company_summary['PAYANN'] = company_summary['PAYANN'].astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5ba8e56",
   "metadata": {},
   "source": [
    "Finally, we need to save our dataframe so it can be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd9a73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_summary.to_csv(\"company_summary_data.csv\", mode = 'w')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e90a807b",
   "metadata": {},
   "source": [
    "We have now created a new data frame called *company_summary* that has been cleaned and is now ready to be used for visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0d1ff",
   "metadata": {},
   "source": [
    "##### Characteristics of Business Owners "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc0a2ff",
   "metadata": {},
   "source": [
    "First, let's import in the necessary modules to pull in the dataset. The **requests** module is needed for the web scrapping via a Census API. The **pandas** module is used to provide the data frame structure in which data is placed in. Four helper functions are also created to extract, transform and load the Census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd9e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get =\"GEO_ID,NAME,NAICS2017,NAICS2017_LABEL,OWNER_SEX,OWNER_SEX_LABEL,OWNER_ETH,OWNER_ETH_LABEL,OWNER_RACE,OWNER_RACE_LABEL,OWNER_VET,OWNER_VET_LABEL,QDESC,QDESC_LABEL,OWNCHAR,OWNCHAR_LABEL,YEAR,OWNPDEMP,OWNPDEMP_F,OWNPDEMP_PCT,OWNPDEMP_PCT_F,OWNPDEMP_S,OWNPDEMP_S_F,OWNPDEMP_PCT_S,OWNPDEMP_PCT_S_F\"\n",
    "get_list = get.split(',')\n",
    "\n",
    "key = \"27133f99dc56a667684ae931bb15f0c829847d6a\"\n",
    "geography = 'us' \n",
    "columns_to_drop = ['GEO_ID', 'NAICS2017', 'OWNER_SEX', 'OWNER_ETH', 'OWNER_RACE', 'OWNER_VET', 'QDESC', 'OWNCHAR', 'OWNPDEMP_PCT', 'OWNPDEMP_PCT_F', 'OWNPDEMP_S', 'OWNPDEMP_S_F', 'OWNPDEMP_PCT_S', 'OWNPDEMP_PCT_S_F', 'us']\n",
    "\n",
    "decoder1 = {'NAME': 'Location',\n",
    "           'NAICS2017_LABEL':'Business_Sector',\n",
    "           'OWNER_SEX_LABEL':'Owner_Sex',\n",
    "           'OWNER_ETH_LABEL':'Owner_Ethnicity',\n",
    "           'OWNER_RACE_LABEL':'Owner_Race',\n",
    "           'OWNER_VET_LABEL':'Veteran_Status',\n",
    "           'QDESC_LABEL':'Question',\n",
    "           'OWNCHAR_LABEL':'Owner_Response',\n",
    "           'YEAR':'Year',\n",
    "           'OWNPDEMP':'Number_of_Responses',\n",
    "            'OWNPDEMP_F':'Response_Footnotes'}\n",
    "\n",
    "def restructure(df):\n",
    "    temp_data= pd.DataFrame(data = df.iloc[1:])\n",
    "    for i in range(len(temp_data.columns)):\n",
    "        temp_data.rename(columns = {i:df.iloc[0][i]}, inplace = True)\n",
    "    return temp_data\n",
    "\n",
    "def extract_data(year):\n",
    "    url = (f\"https://api.census.gov/data/{year}/abscbo?get={get}\" +\n",
    "        f\"&for={geography}:\" +\n",
    "        f\"*&key={key}\")\n",
    "    r = requests.get(url)\n",
    "    text = r.text\n",
    "    data_f = pd.read_json(text)\n",
    "    df = restructure(data_f)\n",
    "    df.to_csv(\"characteristics-of-business-owners-\" + str(year) + \".csv\", mode = 'w')\n",
    "    print(f'Data saved for {year}.')\n",
    "\n",
    "def load_data(year_clean):\n",
    "    df = pd.read_csv('characteristics-of-business-owners-' + str(year_clean) + '.csv')\n",
    "    df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "    print(f'{year_clean} data has been loaded.')\n",
    "    return df\n",
    "\n",
    "def clean_data(dataframe,year):\n",
    "    dataframe.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "    for key, item in decoder1.items():\n",
    "        dataframe.rename(columns = {key: item}, inplace = True)\n",
    "    dataframe.to_csv(\"characteristics-of-business-owners-\" + str(year) + \"_clean.csv\", mode = 'w')\n",
    "    print(f'{year} data has been cleaned,saved and ready to be loaded for use.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec9c4ee",
   "metadata": {},
   "source": [
    "To pull the data from the Census for each year between 2017 to 2020, the *extract_data* function was used to place in the desired variables (which are all of them at this point) into the URL, along with the API Key. Then it uses **request** to pull in data into a json style text and changes it to a data frame. The data frame is then passed through the *restructure* function which takes into account that the first row of the data are column names, not actual data. This data is then saved onto a csv file so that it is not necessary to keep pulling data from the Census website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2dd809",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11024\\2816414235.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2021\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mextract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_data' is not defined"
     ]
    }
   ],
   "source": [
    "#Data saved for 2017, 2018, 2019, 2020\n",
    "for i in range(2017, 2021):\n",
    "    extract_data(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125a1531",
   "metadata": {},
   "source": [
    "The data frame is then loaded from the cvs file for cleaning using the *load_data* function. Our initial data exploration showed that many of the variables provide the same information: one in the form of a code and the other in the form of a description. In addition, some of the variables were deemed irrelevant or non-meaningful. These variables were collected onto a *columns_to_drop* list and used in the *clean_data* function to remove those columns. A dictionary was also created to translate the meaning of the column names assigned by the Census and is used to rename the columns. Lastly, the cleaned data is also saved as a new csv file and ready to be used for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2017,2021):\n",
    "    clean_data(load_data(str(i)),str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_17 = load_data('2017_clean')\n",
    "print(data_17.info())\n",
    "data_18 = load_data('2018_clean')\n",
    "print(data_18.info())\n",
    "data_19 = load_data('2019_clean')\n",
    "print(data_19.info())\n",
    "data_20 = load_data('2020_clean')\n",
    "print(data_20.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d7d175",
   "metadata": {},
   "source": [
    "##### Characteristics of Module Business Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe88a1e",
   "metadata": {},
   "source": [
    "The Module of Business Charachteristics is a dataset accessible via the Annual Business Survey (ABS) API which contains estimates provided for employer firms that responded to a wide-ranging survey conducted by the US Census Bureau in 2021. In order to extract a sufficient amount of data, one must first import the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19528b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e5d00",
   "metadata": {},
   "source": [
    "Next, we will isolate the columns that have deemed useful for the research conducted by our group by explicitly defining them in a list. Notice: the variable \"get\" is a near verbatim copy of the archtypical get statement provided by the ABS for this specific module with the only deviations being the exclusion of variables who's suffixies include include either an \"S\" or the characters \"PCT.\" We have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_columns = ['NAME', 'NAICS2017_LABEL', \"NAICS2017_F\", 'SEX_LABEL', 'ETH_GROUP_LABEL', 'RACE_GROUP_LABEL', 'VET GROUP_LABEL', 'QDESC_LABEL', 'BUSCHAR_LABEL', 'BUSCHAR', 'YEAR', 'FIRMPDEMP', \"FIRMPDEMP_F\", 'RCPPDEMP', \"RCPPDEMP_F\", 'EMP', 'EMP_F', 'EMPSZFI', \"URSZFI\", \"PAYANN\", \"PAYANN_F\"]\n",
    "get = \"GEO_ID,NAME,NAICS2017,NAICS2017_F,NAICS2017_LABEL,SEX,SEX_LABEL,ETH_GROUP_LABEL,RACE_GROUP,RACE_GROUP_LABEL,VET_GROUP,VET_GROUP_LABEL,QDESC,QDESC_LABEL,BUSCHAR,BUSCHAR_LABEL,URSZFI,YEAR,FIRMPDEMP,FIRMPDEMP_F,FIRMPDEMP_PCT,FIRMPDEMP_PCT_F,RCPPDEMP,RCPPDEMP_F,EMP,EMP_F,PAYANN,PAYANN_F\"\n",
    "get_list = get.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b412ec40",
   "metadata": {},
   "source": [
    "Now we shall designate the columns which we intend to exclude from our soon to be constructed Pandas DataFrame():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff4a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending the less useful columns\n",
    "columns_to_drop = []\n",
    "for items in get_list:\n",
    "    if items in useful_columns:\n",
    "        pass\n",
    "    else:\n",
    "        columns_to_drop.append(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72579dd",
   "metadata": {},
   "source": [
    "With the sheep and goats seperated from our list of variables, we will now patch together our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Business Characteristics\n",
    "#========================================================================== \n",
    "#     What was tricky about this dataset was that the API key would not \n",
    "#     accept an open API call without yieleding a non-descript error which\n",
    "#     it would send back to the administrator. However, the API worked fine \n",
    "#     when individual questions from the QDESC_LABEL were called. To work \n",
    "#     around this problem, we will transcribe the QDESC_LIST codes and \n",
    "#     loop through each call to the API, each time appending the new \n",
    "#     results to an established dataframe.\n",
    "# ===========================================================================\n",
    "qdesc_list =    [\n",
    "                \"AREMPT\", \"BUSTARDIFF\", \"BUSTARGETS\", \"CLOUDSERV\", \n",
    "                \"COVIDEFF\", \"COVIDFA\", \"COVIDOP\", \"CREDAPP\", \n",
    "                \"CREDREC\", \"CREDSOUR\", \"CUSSATMON\", \"DEBT\", \n",
    "                \"DIGBUSACT\", \"EMPTRE\", \"EMPTRP\", \"FINUSES\", \n",
    "                \"GOVFAREC\", \"GOVFAREQ\",\"GOVFOR\", \"HEMPTR\", \n",
    "                \"KPIFREQ\", \"KPINUM\", \"PROMOTION\", \"SERVICEPROB\", \n",
    "                \"TECHUSE\", \"UNDERPERFORM\",\n",
    "                ]\n",
    "\n",
    "for i in range (0, len(qdesc_list)):\n",
    "    # Establishing the base case\n",
    "    if i == 0:\n",
    "        key = \"ea9a7f5b5cf50ba715aea3b71320a0ca918b8233\"\n",
    "        geography = 'us'\n",
    "        qdesc = qdesc_list[i]\n",
    "        url = (f\"https://api.census.gov/data/2020/absmcb?get={get}\" +\n",
    "                f\"&for=us:\" +\n",
    "                f\"*&QDESC_LABEL={qdesc}\" +\n",
    "                f\"&key={key}\")\n",
    "        r = requests.get(url)\n",
    "        text = r.text\n",
    "        text\n",
    "        \n",
    "        # We are establishing a temporary dataframe with the text from the API call\n",
    "        df = pd.read_json(text)\n",
    "\n",
    "        # We're now extracting the headers from the 0th row\n",
    "        headers = []\n",
    "        for i in range(0,len(get_list)+ 1):\n",
    "                headers.append(df.iloc[0][i])\n",
    "\n",
    "        # Now we are extracting the rest of the data and updating the headers\n",
    "        module_business_characteristics = pd.DataFrame(data = df.iloc[1:])\n",
    "        for i in range(0,len(get_list)):\n",
    "                module_business_characteristics.rename(columns = {i: headers[i]}, inplace = True)\n",
    "\n",
    "        module_business_characteristics.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "        # Now we will cease operations for 10 seconds so we don't lose connection with the API\n",
    "        time.sleep(10)\n",
    "    else:\n",
    "        # We will repeat the previous steps for the next QDESC_LABEL code\n",
    "        key = \"ea9a7f5b5cf50ba715aea3b71320a0ca918b8233\"\n",
    "        geography = 'us'\n",
    "        qdesc = qdesc_list[i]\n",
    "        url = (f\"https://api.census.gov/data/2020/absmcb?get={get}\" +\n",
    "                f\"&for=us:\" +\n",
    "                f\"*&QDESC_LABEL={qdesc}\" +\n",
    "                f\"&key={key}\")\n",
    "\n",
    "        r = requests.get(url)\n",
    "        text = r.text\n",
    "\n",
    "        df = pd.read_json(text)\n",
    "\n",
    "        headers = []\n",
    "        for i in range(0,len(get_list)+ 1):\n",
    "                headers.append(df.iloc[0][i])\n",
    "\n",
    "        module_business_characteristics_tmp = pd.DataFrame(data = df.iloc[1:])\n",
    "        for i in range(0,len(get_list)):\n",
    "                module_business_characteristics_tmp.rename(columns = {i: headers[i]}, inplace = True)\n",
    "\n",
    "        module_business_characteristics_tmp.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "        # Now we will append this new data to the established dataframe\n",
    "        module_business_characteristics = pd.DataFrame(module_business_characteristics.append(module_business_characteristics_tmp, ignore_index = True))\n",
    "        # Of course, sleep for another 10 seconds to avoid flagging from the API\n",
    "        time.sleep(10)\n",
    "\n",
    "# Execution time: 25mins < Wait Time < 35mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d1154a",
   "metadata": {},
   "source": [
    "As evidenced by the use of the time.sleep() function, one may intuit that this is a large dataset. Indeed, the resulting dataframe contains greater than four million rows of data. Before proceeding further, when one requests information thourgh the ABS API, for every filteration parameter (in the code above we filter results by \"QDESC_LABEL\") one additional column is added. As well, regardless of the number of filtration parameters specified, I have found that the API will always return one additional column. We will now drop these two unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_business_characteristics.drop([28, 29], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c958f",
   "metadata": {},
   "source": [
    "Now that we have successfully imported, and transformed the Module of Business Characteristics dataset, our final step is to save the dataframe in your preferred format. I chose to save this dataframe as a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_business_characteristics.to_csv(\"module_business_characteristics.csv\", mode = 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab726e9",
   "metadata": {},
   "source": [
    "##### Characteristics of a Business"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467acdc",
   "metadata": {},
   "source": [
    "The Census Annual Business Survey conveys information on economic and demographic characteristics for businesses in the United States. In order to work with this particular dataset, we have to import certain packages like requests and beautifulsoup which is necessary for web scrapping and pandas to work with the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2cd4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9fc30c",
   "metadata": {},
   "source": [
    "After importing the necessary packages, we decided to only import columns that would be informational from the census data. Thus, instead of dropping them later on we only copied the columns in the get statement that did not include 'S' at the end or columns that were flagged.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4dc2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_columns = [\"NAME\",\"NAICS2017_LABEL\",\"BUSCHAR_LABEL\",\"BUSCHAR\",\"QDESC_LABEL\",\"YIBSZFI_LABEL\",\"EMP\",\"EMPSZFI_LABEL\",\"FIRMPDEMP\",\"PAYANN\",\"RCPPDEMP\",\"RCPSZFI_LABEL\",\"YEAR\",\"VET_GROUP_LABEL\",\"SEX_LABEL\",\"RACE_GROUP_LABEL\",\"ETH_GROUP_LABEL\"]\n",
    "get = \"GEO_ID,NAME,NAICS2017,NAICS2017_LABEL,SEX,SEX_LABEL,ETH_GROUP,ETH_GROUP_LABEL,RACE_GROUP,RACE_GROUP_LABEL,VET_GROUP,VET_GROUP_LABEL,QDESC,QDESC_LABEL,BUSCHAR,BUSCHAR_LABEL,YIBSZFI_LABEL,EMPSZFI_LABEL,RCPSZFI_LABEL,YEAR,FIRMPDEMP,RCPPDEMP,RCPPDEMP_F,RCPPDEMP_PCT,RCPPDEMP_PCT_F,EMP,EMP_F,EMP_PCT,EMP_PCT_F,PAYANN,PAYANN_F,PAYANN_PCT,PAYANN_PCT_F,FIRMPDEMP_S,FIRMPDEMP_S_F,FIRMPDEMP_PCT_S,FIRMPDEMP_PCT_S_F,RCPPDEMP_S,RCPPDEMP_S_F,RCPPDEMP_PCT_S,RCPPDEMP_PCT_S_F,EMP_S,EMP_S_F,EMP_PCT_S,EMP_PCT_S_F,PAYANN_S,PAYANN_S_F,PAYANN_PCT_S,PAYANN_PCT_S_F\"\n",
    "get_list = get.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d78ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending the less useful columns\n",
    "columns_to_drop = []\n",
    "for items in get_list:\n",
    "    if items in useful_columns:\n",
    "        pass\n",
    "    else:\n",
    "        columns_to_drop.append(items)\n",
    "# When one adds filters to the api, it  adds duplicates of the columns to the data.\n",
    "# Instead of renaming those columns (which are auto-filled in Pandas as integers)\n",
    "# we will append the colmun number to drop it later \n",
    "columns_to_drop.append(49)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9e30c5",
   "metadata": {},
   "source": [
    "For this dataset, the years 2017,2018, 2019, and 2020 were loaded in. Next, the data was read as json then changed into a dataframe. After the remaining data was extracted and the first row was deemed column headers. Lastly, since the data is large time sleep was used so the API won't flag our data. It also took around 15 minutes to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed98e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristics of a Business\n",
    "for i in range(2017,2021):\n",
    "        # Establishing the base case\n",
    "        if i == 2017:\n",
    "                key = \"aefa14e56c15213f9543a02f0f5478021c972174\"\n",
    "                geography = 'us'\n",
    "                url = (f\"https://api.census.gov/data/2017/abscb?get={get}\" +\n",
    "                        f\"&for={geography}:\" +\n",
    "                        f\"*&key={key}\")\n",
    "                r = requests.get(url)\n",
    "                text = r.text\n",
    "\n",
    "                df = pd.read_json(text)\n",
    "\n",
    "                # This selects the 0th-row of the data frame and loads it into an empty list\n",
    "                headers = []\n",
    "                for i in range(0,len(get_list)+ 1):\n",
    "                        headers.append(df.iloc[0][i])\n",
    "\n",
    "                # We're now extracting the remaining data and installing the correct column headers\n",
    "                characteristics_of_a_business = pd.DataFrame(data = df.iloc[1:])\n",
    "                for i in range(0,len(get_list)):\n",
    "                        characteristics_of_a_business.rename(columns = {i: headers[i]}, inplace = True)\n",
    "\n",
    "                characteristics_of_a_business.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "        # Proceeding with the other years\n",
    "        else:\n",
    "                year = str(i)\n",
    "                key = \"aefa14e56c15213f9543a02f0f5478021c972174\"\n",
    "                geography = 'us'\n",
    "                url = (f\"https://api.census.gov/data/{year}/abscb?get={get}\" +\n",
    "                        f\"&for={geography}:\" +\n",
    "                        f\"*&key={key}\")\n",
    "                r = requests.get(url)\n",
    "                text = r.text\n",
    "\n",
    "                df = pd.read_json(text)\n",
    "\n",
    "                headers = []\n",
    "                for i in range(0,len(get_list)+ 1):\n",
    "                        headers.append(df.iloc[0][i])\n",
    "\n",
    "                characteristics_of_a_business_tmp = pd.DataFrame(data = df.iloc[1:])\n",
    "                for i in range(0,len(get_list)):\n",
    "                        characteristics_of_a_business_tmp.rename(columns = {i: headers[i]}, inplace = True)\n",
    "\n",
    "                characteristics_of_a_business_tmp.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "                \n",
    "                # We're appending the next year's data onto the previous year's data frame\n",
    "                characteristics_of_a_business = pd.DataFrame(characteristics_of_a_business.append(characteristics_of_a_business_tmp, ignore_index = True))\n",
    "                \n",
    "                # We'll pause for 5 seconds so the API won't flag us\n",
    "                time.sleep(5)\n",
    "                \n",
    "# Execution Time: 10mins < Wait Time < 15mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a455b18",
   "metadata": {},
   "source": [
    "Finally, the data is loaded and saved as a new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaae633",
   "metadata": {},
   "outputs": [],
   "source": [
    "characteristics_of_a_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070787be",
   "metadata": {},
   "outputs": [],
   "source": [
    "characteristics_of_a_business.to_csv(\"data/characteristics-of-a-business.csv\", mode = 'w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f831df608aecc0293d9af79acc0799bdf4d9356f054bef271c097361b58600a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
